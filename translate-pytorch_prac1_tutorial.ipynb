{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Translating French to English with Pytorch\n",
    "\n",
    "**\n",
    "[Ted Note] Please MUST practice this Pytorch tutorial first to gain clean understanding for simple version\n",
    "**\n",
    "\n",
    "**\n",
    "[Translation with a Sequence to Sequence Network and Attention][1]\n",
    "**\n",
    "\n",
    "[1]: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorchenv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re, pickle, collections, bcolz, numpy as np, keras, sklearn, math, operator \n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorchenv\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path='/data/datasets/fr-en-109-corpus/'\n",
    "#dpath = 'data/translate/'\n",
    "path = '../../../Data/fr-en-109-corpus/'\n",
    "dpath = '../../../Data/translate/'\n",
    "glove_path = '../../../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Prepare corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "The French-English parallel corpus can be downloaded from http://www.statmt.org/wmt10/training-giga-fren.tar. It was created by Chris Callison-Burch, who crawled millions of web pages and then used 'a set of simple heuristics to transform French URLs onto English URLs (i.e. replacing \"fr\" with \"en\" and about 40 other hand-written rules), and assume that these documents are translations of each other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=path+'giga-fren.release2.fixed'\n",
    "en_fname = fname+'.en'\n",
    "fr_fname = fname+'.fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "To make this problem a little simpler so we can train our model more quickly, we'll just learn to translate questions that begin with 'Wh' (e.g. what, why, where which). Here are our regexps that filter the sentences we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52331\n"
     ]
    }
   ],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_fq = re.compile('^([^?.!]+\\?)')\n",
    "\n",
    "lines = ((re_eq.search(eq), re_fq.search(fq)) \n",
    "         for eq, fq in zip(open(en_fname, encoding='utf8'), open(fr_fname, encoding='utf8')))\n",
    "\n",
    "qs = [(e.group(), f.group()) for e, f in lines if e and f]\n",
    "print(len(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is light ?', 'Qu’est-ce que la lumière?'),\n",
       " ('Who are we?', 'Où sommes-nous?'),\n",
       " ('Where did we come from?', \"D'où venons-nous?\"),\n",
       " ('What would we do without it?', 'Que ferions-nous sans elle ?'),\n",
       " ('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',\n",
       "  'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?'),\n",
       " ('What is the major aboriginal group on Vancouver Island?',\n",
       "  'Quel est le groupe autochtone principal sur l’île de Vancouver?')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Because it takes a while to load the data, we save the results to make it easier to load in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(qs, open(path+'fr-en-qs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = pickle.load(open(path+'fr-en-qs.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_qs, fr_qs = zip(*qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Because we are translating at word level, we need to tokenize the text first. (Note that it is also possible to translate at character level, which doesn't require tokenizing.) There are many tokenizers available, but we found we got best results using these simple heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def simple_toks(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['qu’', 'est', 'ce', 'que', 'la', 'lumière', '?'],\n",
       " ['où', 'sommes', 'nous', '?'],\n",
       " [\"d'\", 'où', 'venons', 'nous', '?'],\n",
       " ['que', 'ferions', 'nous', 'sans', 'elle', '?']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_qtoks = list(map(simple_toks, fr_qs)); fr_qtoks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'is', 'light', '?'],\n",
       " ['who', 'are', 'we', '?'],\n",
       " ['where', 'did', 'we', 'come', 'from', '?'],\n",
       " ['what', 'would', 'we', 'do', 'without', 'it', '?']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_qtoks = list(map(simple_toks, en_qs)); en_qtoks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rachel', \"'s\", 'baby', 'is', 'cuter', 'than', 'other', \"'s\", '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_toks(\"Rachel's baby is cuter than other's.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Special tokens used to pad the end of sentences, and to mark the start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0; SOS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Enumerate the unique words (*vocab*) in the corpus, and also create the reverse map (word->index). Then use this mapping to encode every sentence as a list of int indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks2ids(sents):\n",
    "    voc_cnt = collections.Counter(t for sent in sents for t in sent)\n",
    "    vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n",
    "    vocab.insert(PAD, \"<PAD>\")\n",
    "    vocab.insert(SOS, \"<SOS>\")\n",
    "    w2id = {w:i for i,w in enumerate(vocab)}\n",
    "    ids = [[w2id[t] for t in sent] for sent in sents]\n",
    "    return ids, vocab, w2id, voc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_ids, fr_vocab, fr_w2id, fr_counts = toks2ids(fr_qtoks)\n",
    "en_ids, en_vocab, en_w2id, en_counts = toks2ids(en_qtoks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'is', 'light', '?'],\n",
       " ['who', 'are', 'we', '?'],\n",
       " ['where', 'did', 'we', 'come', 'from', '?']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[en_vocab[wid] for wid in s] for s in en_ids[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Stanford's GloVe word vectors can be downloaded from https://nlp.stanford.edu/projects/glove/ (in the code below we have preprocessed them into a bcolz array). We use these because each individual word has a single word vector, which is what we need for translation. Word2vec, on the other hand, often uses multi-word phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(loc):\n",
    "    return (bcolz.open(loc+'.dat')[:],\n",
    "        pickle.load(open(loc+'_words.pkl','rb'), encoding='latin1'),\n",
    "        pickle.load(open(loc+'_wordidx.pkl','rb'), encoding='latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs, en_wv_word, en_wv_idx = load_glove(glove_path+'/glove.6B/results/100d')\n",
    "en_w2v = {w: en_vecs[en_wv_idx[w]] for w in en_wv_word}\n",
    "n_en_vec, dim_en_vec = en_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_w2v['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "For French word vectors, we're using those from http://fauconnier.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec\n",
    "fr_model = word2vec.load(glove_path+'frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin')\n",
    "fr_voc = fr_model.vocab\n",
    "dim_fr_vec = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "We need to map each word index in our vocabs to their word vector. Not every word in our vocabs will be in our word vectors, since our tokenization approach won't be identical to the word vector creators - in these cases we simply create a random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(w2v, targ_vocab, dim_vec):\n",
    "    vocab_size = len(targ_vocab)\n",
    "    emb = np.zeros((vocab_size, dim_vec))\n",
    "    found = 0\n",
    "    \n",
    "    for i, word in enumerate(targ_vocab):\n",
    "        try: emb[i] = w2v[word]; found+=1\n",
    "        except KeyError: emb[i] = np.random.normal(scale=0.6, size=(dim_vec,))\n",
    "            \n",
    "    return emb, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19549, 100), 17251)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embs, found = create_emb(en_w2v, en_vocab, dim_en_vec); en_embs.shape, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26709, 200), 21878)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_embs, found = create_emb(fr_model, fr_vocab, dim_fr_vec); fr_embs.shape, found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Each sentence has to be of equal length. Keras has a convenient function `pad_sequences` to truncate and/or pad each sentence as required - even although we're not using keras for the neural net, we can still use any functions from it we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52331, 30), (52331, 30), (19549, 100), (26709, 200))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 30\n",
    "en_padded = pad_sequences(en_ids, maxlen, 'int64', 'post', 'post')\n",
    "fr_padded = pad_sequences(fr_ids, maxlen, 'int64', 'post', 'post')\n",
    "en_padded.shape, fr_padded.shape, en_embs.shape, fr_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "And of course we need to separate our training and test sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(47097, 30), (5234, 30), (47097, 30), (5234, 30)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "fr_train, fr_test, en_train, en_test = train_test_split(en_padded, fr_padded, test_size=0.1)\n",
    "\n",
    "print([o.shape for o in (fr_train, fr_test, en_train, en_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Here's an example of a French and English sentence, after encoding and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  29, 1418, 5898,   10,    3,   66,   89,   13,   17,    2,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], dtype=int64),\n",
       " array([  19,   13,    4, 1370, 7990,   17, 5395,    6,   45,   30,  365,\n",
       "          14,   27,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], dtype=int64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_train[0], en_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Basic encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_t(arr): return Variable(torch.LongTensor(arr)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_emb_t = torch.FloatTensor(fr_embs).cuda()\n",
    "en_emb_t = torch.FloatTensor(en_embs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(emb_mat, non_trainable=False):\n",
    "    output_size, emb_dim = emb_mat.size() # emb_mat.shape also works\n",
    "    emb = nn.Embedding(output_size, emb_dim)\n",
    "    emb.load_state_dict({'weight': emb_mat})\n",
    "    if non_trainable:\n",
    "        for param in emb.parameters():\n",
    "            param.requires_grad = False\n",
    "    return emb, emb_dim, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Turning a sequence into a representation can be done using an RNN (called the 'encoder'. This approach is useful because RNN's are able to keep track of state and memory, which is obviously important in forming a complete understanding of a sentence.\n",
    "* `bidirectional=True` passes the original sequence through an RNN, and the reversed sequence through a different RNN and concatenates the results. This allows us to look forward and backwards.\n",
    "* We do this because in language things that happen later often influence what came before (i.e. in Spanish, \"el chico, la chica\" means the boy, the girl; the word for \"the\" is determined by the gender of the subject, which comes after)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embs, hidden_size, n_layers=2, n_directions=1):\n",
    "        '''\n",
    "            Args:\n",
    "                embs: in shape (input_size, emb_dim). \n",
    "                      ps. output_size = input_size from encoder perspective\n",
    "        '''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs, True)\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_directions = n_directions\n",
    "        if n_directions == 1:\n",
    "            self.gru = nn.GRU(emb_size, hidden_size, num_layers=n_layers, \n",
    "                          batch_first=True)\n",
    "        elif n_directions == 2:\n",
    "            self.gru = nn.GRU(emb_size, hidden_size, num_layers=n_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "            Args:\n",
    "                - inp: Input of nn.Embedding() \n",
    "                    LongTensor (N, W), \n",
    "                    N = mini-batch, \n",
    "                    W = number of indices to extract per mini-batch\n",
    "            Outputs: Outputs of nn.GRU() \n",
    "                - output: (batch, seq_len, hidden_size * num_directions)\n",
    "                - h_n: (num_layers * num_directions, batch, hidden_size)\n",
    "        '''\n",
    "        return self.gru(self.emb(input), hidden)\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.n_layers*self.n_directions, \n",
    "                                    batch_size, \n",
    "                                    self.hidden_size)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(inp, encoder):\n",
    "    batch_size, input_length = inp.size()\n",
    "    hidden = encoder.initHidden(batch_size)\n",
    "    enc_outputs, hidden= encoder(inp, hidden)\n",
    "    return long_t([SOS]*batch_size), enc_outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we arrive at a vector representation of the sequence which captures everything we need to translate it. We feed this vector into more RNN's, which are trying to generate the labels. After this, we make a classification for what each word is in the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embs, hidden_size, n_layers=2):\n",
    "        '''\n",
    "            Args:\n",
    "                embs: in shape (input_size, emb_dim). \n",
    "                      ps. output_size = input_size from encoder perspective\n",
    "        '''\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs, False)\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers=n_layers, \n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, inp, hidden):\n",
    "        '''\n",
    "            Args:\n",
    "                - inp: Input of nn.Embedding()- \n",
    "                    LongTensor (N, W), \n",
    "                    N = mini-batch, \n",
    "                    W = number of indices to extract per mini-batch\n",
    "        '''\n",
    "        # emb output: (batch_size, emb_dim)\n",
    "        # unsqueeze(1) to turn into (batch_size, 1, emb_dim)\n",
    "        emb = self.emb(inp).unsqueeze(1)\n",
    "        output = emb #F.relu(embedded)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # feed self.out Linear layer by GRU output \n",
    "        # shape (batch_size, seq_len, hidden_size) \n",
    "        # into shape (batch_size, hidden_size)\n",
    "        res = F.log_softmax(self.out(output[:,0]), dim=1)\n",
    "        return res, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.n_layers, \n",
    "                                    batch_size, \n",
    "                                    self.hidden_size)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This graph demonstrates the accuracy decay for a neural translation task. With an encoding/decoding technique, larger input sequences result in less accuracy.\n",
    "\n",
    "<img src=\"https://smerity.com/media/images/articles/2016/bahdanau_attn.png\" width=\"600\">\n",
    "\n",
    "This can be mitigated using an attentional model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "### Attentional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arr(*sz): return torch.randn(sz)/math.sqrt(sz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Var(*sz): return Parameter(Arr(*sz)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embs, hidden_size, n_layers=2, \n",
    "                 dropout_p=0.1, max_length=maxlen,\n",
    "                 n_encoder_directions=1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs, False)\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.n_encoder_directions= n_encoder_directions\n",
    "        \n",
    "        # ** unique for AttnDecoder **\n",
    "        self.attn = nn.Linear(emb_size+self.hidden_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(emb_size+self.hidden_size*self.n_encoder_directions, \n",
    "                                      hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=n_layers, \n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, inp, hidden, encoder_outputs):\n",
    "        '''\n",
    "            Args:\n",
    "                - inp: Input of nn.Embedding()- \n",
    "                    LongTensor (N, W), \n",
    "                    N = mini-batch, \n",
    "                    W = number of indices to extract per mini-batch\n",
    "                ** unique for AttnDecoder ** \n",
    "                - encoder_outputs: (batch_size, self.max_length, hidden_size*n_encoder_directions)\n",
    "                   a -- hidden state output of the Bi-GRU/Bi-LSTM encoder\n",
    "        '''\n",
    "        # emb output: (batch_size, emb_dim)\n",
    "        # [unsqueeze(1) to turn into (batch_size, 1, emb_dim)]\n",
    "        emb = self.emb(inp) #.unsqueeze(1)\n",
    "        emb = self.dropout(emb)\n",
    "        \n",
    "        # concat in input embedded a and hidden s(t-1) \n",
    "        # in shape (batch_size, emb_dim) and (batch_size, hidden_size)\n",
    "        # turn into shape (batch_size, self.max_length) by self.attn Dense Layer\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((emb, hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        # torch.bmm input and output shapes: \n",
    "        # Inputs:\n",
    "        #     - attn_weights: (batch_size, self.max_length)\n",
    "        #     - encoder_outputs: (batch_size, self.max_length, hidden_size*n_encoder_directions)\n",
    "        # Output: \n",
    "        #     - attn_applied: (batch_size, seq_len=1, hidden_size*n_encoder_directions)\n",
    "        #        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs) \n",
    "        \n",
    "        # output shape: (batch_size, emb_dim+hidden_size*n_encoder_directions) \n",
    "        output = torch.cat((emb, attn_applied[:,0]), 1)\n",
    "        \n",
    "        # output shape: (batch_size, hidden_size)\n",
    "        # unsqueeze(1) to turn into (batch_size, 1, hidden_size)\n",
    "        output = self.attn_combine(output).unsqueeze(1)\n",
    "\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        # feed self.out Linear layer by GRU output \n",
    "        # shape (batch_size, seq_len, hidden_size) \n",
    "        # into shape (batch_size, hidden_size)\n",
    "        output = F.log_softmax(self.out(output[:,0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.n_layers, \n",
    "                                    batch_size, \n",
    "                                    self.hidden_size)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "### Encoder, Decoder, Attention testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "Pytorch makes it easy to check intermediate results, when creating a custom architecture such as this one, since you can interactively run each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=16):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30])\n",
      "torch.Size([4, 30, 200])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 4\n",
    "fra, eng = get_batch(fr_train, en_train, batch_size)\n",
    "inp = long_t(fra)\n",
    "targ = long_t(eng)\n",
    "\n",
    "emb, emb_size, output_size = create_emb(fr_emb_t, True)\n",
    "emb.cuda()\n",
    "print(inp.size()) # shape: (batch_size, seq_len)\n",
    "print(emb(inp).shape) # shape: (batch_size, seq_len, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(fr_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = encoder.initHidden(batch_size).cuda()\n",
    "gru = nn.GRU(emb_size, hidden_size, num_layers=2, \n",
    "                          batch_first=True).cuda()\n",
    "enc_outputs, hidden = gru(emb(inp), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4, 30, 128])\n",
      "torch.Size([2, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "dec_input, enc_outputs, hidden = encode(inp, encoder)\n",
    "print(dec_input.shape) # (batch_size,)\n",
    "print(enc_outputs.shape) # (batch_size, seq_len, hidden_size)\n",
    "print(hidden.shape) # (n_layers, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.n_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(en_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = decoder.initHidden(batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_outputs, hidden = decoder(dec_input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dec_input.shape); print(dec_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 19549])\n"
     ]
    }
   ],
   "source": [
    "print(dec_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "print(enc_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_decoder = AttnDecoderRNN(en_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = attn_decoder.initHidden(batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_outputs, hidden, attn_weights = attn_decoder(dec_input, hidden, enc_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 19549])\n",
      "torch.Size([2, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "print(dec_outputs.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "topv, topi = dec_outputs.data.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14434, 14434, 14434, 14434]\n",
      "Variable containing:\n",
      " 14434\n",
      " 14434\n",
      " 14434\n",
      " 14434\n",
      "[torch.cuda.LongTensor of size 4 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 14434\n",
      " 14434\n",
      " 14434\n",
      " 14434\n",
      "[torch.cuda.LongTensor of size 4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ni = [topi[b][0] for b in range(batch_size)]; \n",
    "print(ni); print(long_t(ni)); print(long_t(ni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 14434\n",
       " 14434\n",
       " 14434\n",
       " 14434\n",
       "[torch.cuda.LongTensor of size 4x1 (GPU 0)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_t(ni).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since    # seconds since starting\n",
    "    es = s / (percent) # seconds expecting finish all iters\n",
    "    rs = es - s        # seconds remaining to expecting finish\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pytorch has limited functionality for training models automatically - you will generally have to write your own training loops. However, Pytorch makes it far easier to customize how this training is done, such as using *teacher forcing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, \n",
    "                     encoder_optimizer, decoder_optimizer, \n",
    "                      criterion, max_length=maxlen,  \n",
    "                      teacher_forcing_ratio=0.5, batch_size=32): \n",
    "    decoder_input, encoder_outputs, hidden = encode(input_variable, encoder)\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    #input_length = input_variable.size()[1]\n",
    "    target_length = target_variable.size()[1]\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    if encoder.n_directions==1:\n",
    "        decoder_hidden = hidden\n",
    "    elif encoder.n_directions==2:\n",
    "        decoder_hidden = decoder.initHidden(batch_size) \n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[:, di])\n",
    "            decoder_input = target_variable[:, di]  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1) # return top (values, indices)\n",
    "            ni = [topi[b][0] for b in range(batch_size)] #[0][0] # topi shape: (batch_size, 1) \n",
    "            \n",
    "            # *** why pass topi indices as decoder_input? ***\n",
    "            # ==> Argmax to get the max propability Index\n",
    "            decoder_input = long_t(ni) #Variable(torch.LongTensor(ni)) # Variable(torch.LongTensor([[ni]]))\n",
    "            \n",
    "            #decoder_input = decoder_input.cuda() # if use_cuda else decoder_input\n",
    "            \n",
    "            loss += criterion(decoder_output, target_variable[:, di])\n",
    "            #if ni == EOS_token: \n",
    "            #    break\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_grad_params(o):\n",
    "    return (p for p in o.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpochs(encoder, decoder, n_epochs, \n",
    "               print_every=50, plot_every=25, \n",
    "               learning_rate=0.01, \n",
    "                batch_size=32):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "        \n",
    "    encoder_optimizer = optim.RMSprop(req_grad_params(encoder), lr=learning_rate)\n",
    "    decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    n = len(fr_train)\n",
    "    n_batch = 0\n",
    "    n_batch_total = (n % batch_size) * n_epochs \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for batch in range(0, n % batch_size):\n",
    "            \n",
    "            fra, eng = get_batch(fr_train, en_train, batch_size)\n",
    "            input_variable = long_t(fra)\n",
    "            target_variable = long_t(eng)\n",
    "\n",
    "            # use_teacher_forcing if running % below teacher_forcing_ratio\n",
    "            '''\n",
    "            if (n_batch/n_batch_total) < 0.5:\n",
    "                teacher_forcing_ratio = 1.0\n",
    "            elif (n_batch/n_batch_total) < 0.8:\n",
    "                teacher_forcing_ratio = 0.5\n",
    "            else: \n",
    "                teacher_forcing_ratio = 0.0\n",
    "            '''\n",
    "            loss = train(input_variable, target_variable, encoder, decoder, \n",
    "                         encoder_optimizer, decoder_optimizer, criterion, \n",
    "                        teacher_forcing_ratio=1.0,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            n_batch += 1\n",
    "\n",
    "            if n_batch % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, n_batch / n_batch_total), \n",
    "                                            epoch, (n_batch / n_batch_total * 100),  \n",
    "                                            print_loss_avg))\n",
    "            if n_batch % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "encoder = EncoderRNN(fr_emb_t, hidden_size).cuda()\n",
    "decoder = AttnDecoderRNN(en_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 21s (- 90m 51s) (2 0%) 50.2216\n",
      "0m 42s (- 88m 1s) (4 0%) nan\n",
      "1m 8s (- 93m 42s) (6 1%) nan\n",
      "1m 31s (- 94m 8s) (8 1%) nan\n",
      "1m 52s (- 92m 10s) (10 2%) nan\n",
      "2m 13s (- 90m 42s) (12 2%) nan\n",
      "2m 34s (- 89m 37s) (14 2%) nan\n",
      "2m 55s (- 88m 38s) (16 3%) nan\n",
      "3m 23s (- 90m 37s) (18 3%) nan\n",
      "3m 43s (- 89m 33s) (20 4%) nan\n",
      "4m 4s (- 88m 42s) (22 4%) nan\n",
      "4m 25s (- 87m 55s) (24 4%) nan\n",
      "4m 47s (- 87m 15s) (26 5%) nan\n",
      "5m 12s (- 87m 40s) (28 5%) nan\n",
      "5m 33s (- 87m 0s) (30 6%) nan\n",
      "5m 54s (- 86m 24s) (32 6%) nan\n",
      "6m 15s (- 85m 50s) (34 6%) nan\n",
      "6m 37s (- 85m 18s) (36 7%) nan\n",
      "6m 58s (- 84m 49s) (38 7%) nan\n",
      "7m 20s (- 84m 20s) (40 8%) nan\n",
      "7m 43s (- 84m 18s) (42 8%) nan\n",
      "8m 4s (- 83m 44s) (44 8%) nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-041df8f0fae7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainEpochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-e23bc00f25d8>\u001b[0m in \u001b[0;36mtrainEpochs\u001b[1;34m(encoder, decoder, n_epochs, print_every, plot_every, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m                          \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                         \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                         batch_size=batch_size)\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-9b33acb1120f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, teacher_forcing_ratio, batch_size)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;31m#    break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainEpochs(encoder, decoder, 500, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
