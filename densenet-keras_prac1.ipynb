{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "import utils2_ted; reload(utils2_ted)\n",
    "from utils2_ted import *\n",
    "import utils2; reload(utils2)\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a Keras implementation of Huang et al.'s [DenseNet](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "Our motivation behind studying DenseNet is because of how well it works with limited data.\n",
    "\n",
    "DenseNet beats state-of-the-art results on CIFAR-10/CIFAR-100 w/ and w/o data augmentation, but the performance increase is most pronounced w/o data augmentation.\n",
    "\n",
    "Compare to FractalNet, state-of-the-art on both datasets:\n",
    "* CIFAR-10: ~ 30 % performance increase w/ DenseNet\n",
    "* CIFAR-100: ~ 30 % performance increase w/ DenseNet\n",
    "\n",
    "That increase is motivation enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is a DenseNet?\n",
    "\n",
    "Put simply, DenseNet is a Resnet where we replace addition with concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in broad terms, a Resnet is a Convnet that uses residual block structures.\n",
    "\n",
    "These \"blocks\" work as follows:\n",
    "* Let L<sub>t</sub> be the input layer to block\n",
    "* Perform conv layer transformations/activations on L<sub>t</sub>, denote by f(<sub>t</sub>)\n",
    "* Call output layer of block L<sub>t+1</sub>\n",
    "* Define L<sub>t+1</sub> = f(L<sub>t</sub>)+ L<sub>t</sub>  \n",
    "    * That is, total output is the conv layer outputs plus the original input\n",
    "* We call residual block b.c. f(L<sub>t</sub>)=L<sub>t+1</sub> - L<sub>t</sub>, the residual\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the difference w/ DenseNet is instead of adding L<sub>t</sub> to L<sub>t+1</sub>, it is being concatenated.\n",
    "\n",
    "As with Resnet, DenseNet consists of multiple blocks.\n",
    "Therefore, there is a recursive relationship across blocks:\n",
    "* Block B<sub>i</sub> takes as input the ouput of block B<sub>i-1</sub> concatenated with the input of B<sub>i-1</sub>\n",
    "* The input to B<sub>i-1</sub> is the ouput of block B<sub>i-2</sub> concatenated with the input of B<sub>i-2</sub>\n",
    "* So on and so forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of filters added to each layer needs to be monitored, given that the input space for each block keeps growing.\n",
    "\n",
    "Huang et al. calls the # of filters added at each layer the *growth rate*, and appropriately denotes this number with the related letter *k*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densenet / CIFAR 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((50000, 32, 32, 3), (50000, 1)), ((10000, 32, 32, 3), (10000, 1)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to normalize pixel values (0-255) to unit interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_class = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "CIFAR_label = {i:c for i, c in enumerate(CIFAR_class)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automobile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1aa80015940>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n78NzaabVgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa90cfb4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 4\n",
    "print(CIFAR_class[int(np.squeeze(y_train[i]))])\n",
    "plt.imshow(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some helper functions for piecing together our network using Keras' Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These components should all be familiar to you:\n",
    "* Relu activation\n",
    "* Dropout regularization\n",
    "* Batch-normalization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#def relu(x): return relu(x) # Activation('relu')(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer:\n",
    "* L2 Regularization\n",
    "* 'same' border mode returns same width/height\n",
    "* Pass output through Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ConvBlock as sequence:\n",
    "* Batchnorm\n",
    "* ReLU Activation\n",
    "* Conv layer (conv w/ Dropout)\n",
    "\n",
    "The authors also use something called a *bottleneck* layer to reduce dimensionality of inputs. \n",
    "\n",
    "Recall that the filter space dimensionality grows at each block. The input dimensionality will determine the dimensionality of your convolution weight matrices, i.e. # of parameters.\n",
    "\n",
    "At size 3x3 or larger, convolutions can become extremely costly and # of parameters can increase quickly as a function of the input feature (filter) space. Therefore, a smart approach is to reduce dimensionality of filters by using a 1x1 convolution w/ smaller # of filters before the larger convolution.\n",
    "\n",
    "Bottleneck consists of:\n",
    "* 1x1 conv\n",
    "* Compress # of filters into growth factor `nf` * 4\n",
    "* Batchnorm -> ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, nf, bottleneck=False, p=None, wd=0.):\n",
    "    \n",
    "    if bottleneck:\n",
    "        x = Activation('relu')(BatchNormalization()(x))\n",
    "        x = Conv2D(nf*4, (1,1), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(wd))(x)\n",
    "        x = Dropout(p)(x)\n",
    "        \n",
    "    x = Activation('relu')(BatchNormalization()(x))\n",
    "    x = Conv2D(nf, (3,3), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(wd))(x)\n",
    "    x = Dropout(p)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the dense block:\n",
    "* Take given input `x`\n",
    "* Pass through a conv block for output `b`\n",
    "* Concatenate input `x` and conv block output `b`\n",
    "* Set concatenation as new input `x` for next block\n",
    "* Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, n_layers, growth_rate, bottleneck=False, p=None, wd=0.):\n",
    "    if bottleneck: n_layers//=2\n",
    "    \n",
    "    for l in range(n_layers):\n",
    "        b = conv_block(x, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        x = Concatenate(axis=-1)([x, b])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As typical for CV architectures, we'll do some pooling after computation.\n",
    "\n",
    "We'll define this unit as the transition block, and we'll put one between each dense block.\n",
    "\n",
    "Aside from BN -> ReLU and Average Pooling, there is also an option for filter *compression* in this block. This is simply feature reduction via 1x1 conv as discussed before, where the new # of filters is a percentage of the incoming # of filters.\n",
    "\n",
    "Together with bottleneck, compression has been shown to improve performance and computational efficiency of DenseNet architectures. (the authors call this DenseNet-BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(x, compression=1.0, p=None, wd=0.):\n",
    "    nf = int(int(x.get_shape().as_list()[-1]) * compression)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(nf, (1,1), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(wd))(x)\n",
    "    x = Dropout(p)(x)\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the DenseNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now defined all the building blocks (literally) to put together a DenseNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nb_classes: number of classes\n",
    "- img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
    "- depth: total number of layers \n",
    "    - Includes 4 extra non-block layers\n",
    "        - 1 input conv, 3 output layers\n",
    "- nb_block: number of dense blocks (generally = 3). \n",
    "    - NOTE: Layers / block are evenly allocated. Therefore nb_block must be a factor of (Depth - 4)\n",
    "- growth_rate: number of filters to add per dense block\n",
    "- nb_filter:  initial number of filters\n",
    "- bottleneck: add bottleneck blocks\n",
    "- Compression: Filter compression factor in transition blocks.\n",
    "- p: dropout rate\n",
    "- wd: weight decay\n",
    "- activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'. Note that if sigmoid is used, classes must be 1.\n",
    "\n",
    "Returns: keras tensor with nb_layers of conv_block appended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From start to finish, this generates:\n",
    "* Conv input layer\n",
    "* Alternate between Dense/Transition blocks `nb_block` times, ommitting Transition block after last Dense block\n",
    "    * Each Dense block has `(Depth-4)/nb_block` layers\n",
    "* Pass final Dense block to BN -> ReLU\n",
    "* Global Avg Pooling\n",
    "* Dense layer w/ desired output activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_net(n_classes, img_input, depth=40, n_block=3, \n",
    "     growth_rate=12, n_filter=16, bottleneck=False, compression=1.0, p=None, wd=0, activation='softmax'):\n",
    "    \n",
    "    assert activation=='softmax' or activation=='sigmoid'\n",
    "    assert (depth - 4) % n_block == 0\n",
    "    \n",
    "    n_layers_per_block = int((depth - 4) / n_block)\n",
    "    n_layers = [n_layers_per_block] * n_block\n",
    "    \n",
    "    x = Conv2D(n_filter, (3,3), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(wd))(img_input)\n",
    "    x = Dropout(p)(x)\n",
    "    \n",
    "    for i, block in enumerate(n_layers):\n",
    "        x = dense_block(x, block, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        if i != len(n_layers)-1:\n",
    "            x = transition_block(x, compression=compression, p=p, wd=wd)\n",
    "    \n",
    "    x = Activation('relu')(BatchNormalization()(x))\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(n_classes, activation=activation, kernel_regularizer=l2(wd))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test it out on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]; input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_block nf: 104\n",
      "transition_block nf: 148\n"
     ]
    }
   ],
   "source": [
    "x = create_dense_net(10, img_input, depth=100, #n_block=3, growth_rate=12, activation='softmax'\n",
    "     n_filter=16, bottleneck=True, compression=0.5, p=1e-4, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(img_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.SGD(0.1, 0.9, nesterov=True), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "parms = {'verbose': 2, 'callbacks': [TQDMNotebookCallback()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will likely need to run overnight + lr annealing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1fdcbf315446a687d54c373f1cc19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f43a442e6441938d2c13f7aa730bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, 64, 20, validation_data=(X_test, y_test), **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
