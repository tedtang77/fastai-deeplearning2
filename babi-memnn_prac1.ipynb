{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babi End to End MemNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from importlib import reload \n",
    "import utils2; reload(utils2)\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A memory network is a network that can retain information; it can be trained on a structured story and will learn how to answer questions about said story.\n",
    "\n",
    "This notebook contains an implementation of an end-to-end memory network trained on the Babi tasks dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from this section is mainly taken from the babi-memnn example in the keras repo.\n",
    "\n",
    "* [Popular Science](http://www.popsci.com/facebook-ai)\n",
    "* [Slate](http://www.slate.com/blogs/future_tense/2016/06/28/facebook_s_ai_researchers_are_making_bots_smarter_by_giving_them_memory.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Babi dataset is a collection of tasks (or stories) that detail events in a particular format. At the end of each task is a question with a labelled answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to construct the dataset from the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parser formats the story into a time-order labelled sequence of sentences, followed by the question and the labelled answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        if int(nid) == 1: story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            substory = [[str(i)+\":\"]+x for i,x in enumerate(story) if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else: story.append(tokenize(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'challenge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-076b101c649c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Understanding parse_stories(lines)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchallenge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'challenge' is not defined"
     ]
    }
   ],
   "source": [
    "# Understanding parse_stories(lines) \n",
    "f = tar.extractfile(challenge.format('train'))\n",
    "lines = f.readlines()\n",
    "print(len(lines))\n",
    "\n",
    "data = []\n",
    "story = []\n",
    "for line in lines[:30]:\n",
    "    line = line.decode('utf-8').strip()\n",
    "    nid, line = line.split(' ', 1)\n",
    "    if int(nid) == 1: story = []\n",
    "    print(nid, line)\n",
    "    if '\\t' in line:\n",
    "        q, a, supporting = line.split('\\t')\n",
    "        q = tokenize(q)\n",
    "        substory = None\n",
    "        substory = [[str(i)+\":\"]+x for i, x in enumerate(story) if x]\n",
    "        data.append( (substory, q, a) )\n",
    "        story.append('')\n",
    "        #print(q, a, supporting)\n",
    "        print(\"substory:\", substory)\n",
    "    else:\n",
    "        story.append(tokenize(line))\n",
    "        \n",
    "#print(\"\\nDATA:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we download and parse the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = get_file('babi-tasks-v1-2.tar.gz', \n",
    "                origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "tar = tarfile.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "    'two_supporting_facts_1k': 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt',\n",
    "}\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "# challenge_type = 'two_supporting_facts_10k'\n",
    "challenge = challenges[challenge_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stories(f):\n",
    "    data = parse_stories(f.readlines())\n",
    "    return [(story, q, answer) for story, q, answer in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate upper bounds for things like words in sentence, sentences in a story, etc. for the corpus, which will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories = train_stories + test_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['0:', 'Mary', 'moved', 'to', 'the', 'bathroom', '.'],\n",
       "  ['1:', 'John', 'went', 'to', 'the', 'hallway', '.'],\n",
       "  ['3:', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['4:', 'Sandra', 'moved', 'to', 'the', 'garden', '.']],\n",
       " ['Where', 'is', 'Daniel', '?'],\n",
       " 'hallway')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "story_maxlen = max(len(s) for x, _, _ in stories for s in x)\n",
    "story_maxsents = max(len(x) for x, _, _ in stories)\n",
    "query_maxlen = max(len(x) for _, x, _ in stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_flatten(el):\n",
    "    return isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes))\n",
    "\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if do_flatten(el): yield from flatten(el)\n",
    "        else: yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary of corpus and find size, including a padding element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(flatten(stories)))\n",
    "vocab.insert(0, '<PAD>')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 8, 4, 10000, 1000)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_maxsents, vocab_size, story_maxlen, query_maxlen, len(train_stories), len(test_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is in the correct format.\n",
    "\n",
    "Each task in the dataset contains a list of tokenized sentences ordered in time, followed by a question about the story with a given answer.\n",
    "\n",
    "In the example below, we go can backward through the sentences to find the answer to the question \"Where is Daniel?\" as sentence 12, the last sentence to mention Daniel.\n",
    "\n",
    "This task structure is called a **\"one supporting fact\"** structure, which means that we only need to find one sentence in the story to answer our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['0:', 'Mary', 'moved', 'to', 'the', 'office', '.'],\n",
       "  ['1:', 'John', 'moved', 'to', 'the', 'garden', '.'],\n",
       "  ['3:', 'Sandra', 'moved', 'to', 'the', 'bedroom', '.'],\n",
       "  ['4:', 'Sandra', 'went', 'back', 'to', 'the', 'office', '.'],\n",
       "  ['6:', 'John', 'went', 'to', 'the', 'bedroom', '.'],\n",
       "  ['7:', 'John', 'journeyed', 'to', 'the', 'garden', '.'],\n",
       "  ['9:', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['10:', 'John', 'journeyed', 'to', 'the', 'bedroom', '.'],\n",
       "  ['12:', 'Daniel', 'journeyed', 'to', 'the', 'bathroom', '.'],\n",
       "  ['13:', 'John', 'travelled', 'to', 'the', 'garden', '.']],\n",
       " ['Where', 'is', 'Daniel', '?'],\n",
       " 'bathroom')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stories[534]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an index mapping for the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx = dict((c,i) for i,c in enumerate(vocab))\n",
    "#word_idx = {c:i for i,c in enumerate(vocab)}　#same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we vectorize our dataset by mapping words to their indices. We enforce consistent dimension by padding vectors up to the upper bounds we calculated earlier with our pad element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []; Xq = []; Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [[word_idx[w] for w in s] for s in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = [word_idx[answer]]\n",
    "        X.append(x); Xq.append(xq); Y.append(y)\n",
    "    return ([pad_sequences(x, maxlen=story_maxlen) for x in X],\n",
    "             pad_sequences(Xq, maxlen=query_maxlen),\n",
    "             np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, (6, 8), (8, 8))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs_train), inputs_train[2].shape, inputs_train[-2].shape  # inputs_train shape: 10000 * [sent, 8] arrays  \n",
    "                                                                  # m = len(train_stories): 10000\n",
    "                                                                  # story_maxlen (maximu words per sentence): 8\n",
    "                                                                  # sent (per story): not fixed number < story_maxsents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2, 16, 30, 29, 28, 27,  1],\n",
       "       [ 0,  6, 16, 31, 29, 28, 19,  1]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stack_inputs(inputs):\n",
    "    for i, it in enumerate(inputs):\n",
    "        inputs[i] = np.concatenate([it, \n",
    "                           np.zeros((story_maxsents-it.shape[0],story_maxlen), 'int')])\n",
    "    return np.stack(inputs)\n",
    "\n",
    "inputs_train = stack_inputs(inputs_train)\n",
    "inputs_test = stack_inputs(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2, 16, 30, 29, 28, 27,  1],\n",
       "       [ 0,  6, 16, 31, 29, 28, 19,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10, 8)\n",
      "(1000, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "print(inputs_train.shape) # inputs_train shape: [m = len(train_stories), story_maxsents, story_maxlen] arrays       \n",
    "print(inputs_test.shape)  # inputs_test shape: [m = len(test_stories), story_maxsents, story_maxlen] arrays                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 4), (1000, 4))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(queries_train.shape) # queries_train shape: [m = len(train_stories), query_maxlen] arrays       \n",
    "print(queries_test.shape) # queries_test shape: [m = len(train_stories), query_maxlen] arrays    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs for keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inps = [inputs_train, queries_train]  # train dataset = [inputs_train, queries_train]\n",
    "val_inps = [inputs_test, queries_test] # val dataset = [inputs_test, queries_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1), (1000, 1))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(answers_train.shape)\n",
    "print(answers_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach to solving this task relies not only on word embeddings, but sentence embeddings.\n",
    "\n",
    "The authors of the Babi paper constructed sentence embeddings by simply adding up the word embeddings; this might seem naive, but given the relatively small length of these sentences we can expect the sum to capture relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 20\n",
    "params = {'verbose': 2, 'callbacks': [TQDMNotebookCallback(leave_inner=False)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **[TimeDistributed](https://keras.io/layers/wrappers/)** here to apply the embedding to every element of the sequence, then the <tt>Lambda</tt> layer adds them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emb_sent_bow(inp):\n",
    "    emb = TimeDistributed(Embedding(vocab_size, emb_dim))(inp) # emb.shape: [m, story_maxsents=10, story_maxlen=8, emb_dim=20]\n",
    "                                                               # TimeDistributed() seems to be for this first inp dim: 'story_maxsents=10'\n",
    "    #print('emb.shape:', repr(emb.shape))\n",
    "    return Lambda(lambda x: K.sum(x, axis=2))(emb)    # the Lambda layer adds up all embedding (in shape [m, story_maxsents=10, emb_dim=20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding works as desired; the raw input has 10 sentences of 8 words, and the output has 10 sentence embeddings of length 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_story.shape: TensorShape([Dimension(None), Dimension(10), Dimension(8)])\n",
      "emb_story.shape: TensorShape([Dimension(None), Dimension(10), Dimension(20)])\n"
     ]
    }
   ],
   "source": [
    "inp_story = Input((story_maxsents, story_maxlen))  # TimeDistributed() is for this first inp_story dim: 'story_maxsents=10'\n",
    "emb_story = emb_sent_bow(inp_story)\n",
    "print('inp_story.shape:', repr(inp_story.shape))  # inp_story.shape: [m, story_maxsents=10, story_maxlen=8]\n",
    "print('emb_story.shape:', repr(emb_story.shape))  # emb_story.shape: [m, story_maxsents=10, emb_dim=20    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the queries, omitting the <tt>TimeDistributed</tt> since there is only one query. We use <tt>Reshape</tt> to match the rank of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_q.shape: TensorShape([Dimension(None), Dimension(4)])\n",
      "emb_q.shape TensorShape([Dimension(None), Dimension(1), Dimension(20)])\n"
     ]
    }
   ],
   "source": [
    "inp_q = Input((query_maxlen,))\n",
    "emb_q = Embedding(vocab_size, emb_dim)(inp_q) # emb_q.shape: [m, query_maxlen=4, emb_dim=20]\n",
    "#print('emb_q.shape', repr(emb_q.shape))\n",
    "emb_q = Lambda(lambda x: K.sum(x, axis=1))(emb_q) # # the Lambda layer adds up all embedding (in shape [m, emb_dim=20])\n",
    "#print('emb_q.shape', repr(emb_q.shape))\n",
    "emb_q = Reshape((1, emb_dim))(emb_q)\n",
    "print('inp_q.shape:', repr(inp_q.shape)) \n",
    "print('emb_q.shape', repr(emb_q.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual memory network is incredibly simple.\n",
    "\n",
    "* For each story, we take the dot product of every sentence embedding with that story's query embedding. This gives us a list of numbers proportional to how similar each sentence is with the query.\n",
    "* We pass this vector of dot products through a softmax function to return a list of scalars that sum to one and tell us how similar the query is to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([Dimension(None), Dimension(10), Dimension(1)])\n"
     ]
    }
   ],
   "source": [
    "x = Dot(axes=2)([emb_story, emb_q]) \n",
    "#print(repr(x.shape))              # x.shape: [m, story_maxsents=10, 1]  \n",
    "x = Reshape((story_maxsents,))(x)  # Softmax works on the last dimension, so I have to Reshape to get rid of the unit axis \n",
    "                                   # and then I Reshape again to put the unit axis back on again.\n",
    "x = Activation('softmax')(x)\n",
    "#print(repr(x.shape))              # x.shape: [m, story_maxsents=10]  \n",
    "match = Reshape((story_maxsents, 1))(x)  # match: how similar the query is to each sentence.\n",
    "print(repr(match.shape))     # match.shape: [m, story_maxsents=10, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we construct a second, separate, embedding function for the sentences\n",
    "* We then take the weighted average of these embeddings, using the softmax outputs as weights\n",
    "* Finally, we pass this weighted average though a dense layer and classify it w/ a softmax into one of the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([Dimension(None), Dimension(1), Dimension(20)])\n"
     ]
    }
   ],
   "source": [
    "emb_c = emb_sent_bow(inp_story)  # emb_c.shape: [m, story_maxsents=10, emb_dim=20]\n",
    "x = Dot(axes=1)([match, emb_c])\n",
    "print(repr(x.shape))             # x.shape: [m, 1, emb_dim=20]\n",
    "response = Reshape((emb_dim,))(x)  # reshape to get rid of the unit axis, so that we can stick it \n",
    "                                   # through a dense layer with a softmax and that gives us our final result.\n",
    "res = Dense(vocab_size, activation='softmax')(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer = Model([inp_story, inp_q], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it works extremely well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cef711da5b4562a51ecfae26d4780a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec5c1e17c9a48e49b39ed7bfd829857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2s - loss: 0.4782 - acc: 0.8343 - val_loss: 0.0395 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d918a0e215d4d41b9e0add33c703907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4\n",
      "0s - loss: 0.0068 - acc: 0.9981 - val_loss: 7.7516e-04 - val_acc: 0.9990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6271e45720d644e886172432e8487af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "0s - loss: 0.0054 - acc: 0.9989 - val_loss: 1.1262e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665cc46d913f40faa5209ede66b87611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4\n",
      "0s - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0018 - val_acc: 0.9990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.set_value(answer.optimizer.lr, 1e-2)\n",
    "hist = answer.fit(inps, answers_train, **params, epochs=4, batch_size=batch_size,\n",
    "                 validation_data=(val_inps, answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look inside our model to see how it's weighting the sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = Model([inp_story, inp_q], match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qnum = 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['0:', 'Sandra', 'travelled', 'to', 'the', 'office', '.'],\n",
       "  ['1:', 'Sandra', 'went', 'to', 'the', 'bathroom', '.'],\n",
       "  ['3:', 'Mary', 'went', 'to', 'the', 'bedroom', '.'],\n",
       "  ['4:', 'Daniel', 'moved', 'to', 'the', 'hallway', '.']],\n",
       " ['Where', 'is', 'Sandra', '?'],\n",
       " 'bathroom')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_st = len(train_stories[qnum][0]); print(l_st)\n",
    "train_stories[qnum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, for the question \"Where is Sandra?\", the largest weight is the last sentence with the name Sandra, sentence 1 with 0.98.\n",
    "\n",
    "The second highest is of course the first sentence, which also mentions Sandra. But the model has learned that the last occurring sentence is what is important; this is why we added the counter at the beginning of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.1552e-02, 9.5723e-01, 5.3446e-05, 1.1608e-03], dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(f.predict([inputs_train[qnum:qnum+1], queries_train[qnum:qnum+1]]))[:l_st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_train[qnum:qnum+1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.9062e-11, 1.0200e-10, 7.7399e-11, 5.3735e-11, 7.6401e-11,\n",
       "        6.7729e-11, 1.8837e-10, 1.8352e-10, 1.3282e-10, 1.4026e-10,\n",
       "        1.0462e-10, 1.9415e-10, 1.0147e-10, 3.3772e-11, 1.3133e-10,\n",
       "        4.3228e-11, 5.0356e-11, 1.2204e-10, 1.2993e-10, 1.0000e+00,\n",
       "        2.5772e-08, 6.1652e-09, 9.5249e-07, 1.5805e-10, 3.1113e-11,\n",
       "        1.0092e-07, 1.0156e-10, 4.2901e-08, 1.0828e-11, 1.0363e-10,\n",
       "        2.7131e-10, 1.0985e-10]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.predict([inputs_train[qnum:qnum+1], queries_train[qnum:qnum+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bathroom'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: ['Where', 'is', 'Sandra', '?']\n",
      "predict answer: ['1:', 'Sandra', 'went', 'to', 'the', 'bathroom', '.']\n",
      "real answer: bathroom\n"
     ]
    }
   ],
   "source": [
    "answer_idx = np.argmax(np.squeeze(f.predict([inputs_train[qnum:qnum+1], queries_train[qnum:qnum+1]]))[:l_st])\n",
    "print('question:', train_stories[qnum][1])\n",
    "print('predict answer:', train_stories[qnum][0][answer_idx])\n",
    "\n",
    "print('real answer:', vocab[int(answers_train[qnum:qnum+1][0])] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi hop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at an example of a two-supporting fact story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "    'two_supporting_facts_1k': 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt',\n",
    "}\n",
    "#challenge_type = 'single_supporting_fact_10k'\n",
    "challenge_type = 'two_supporting_facts_10k'\n",
    "challenge = challenges[challenge_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['0:', 'Mary', 'went', 'to', 'the', 'hallway', '.'],\n",
       "  ['1:', 'Daniel', 'went', 'back', 'to', 'the', 'bedroom', '.'],\n",
       "  ['2:', 'Sandra', 'went', 'back', 'to', 'the', 'garden', '.'],\n",
       "  ['3:', 'Mary', 'went', 'to', 'the', 'office', '.'],\n",
       "  ['4:', 'Mary', 'journeyed', 'to', 'the', 'kitchen', '.'],\n",
       "  ['5:', 'Sandra', 'moved', 'to', 'the', 'office', '.'],\n",
       "  ['6:', 'Sandra', 'journeyed', 'to', 'the', 'hallway', '.'],\n",
       "  ['7:', 'Daniel', 'journeyed', 'to', 'the', 'garden', '.'],\n",
       "  ['8:', 'Mary', 'journeyed', 'to', 'the', 'bathroom', '.'],\n",
       "  ['9:', 'John', 'went', 'back', 'to', 'the', 'bathroom', '.'],\n",
       "  ['10:', 'Sandra', 'travelled', 'to', 'the', 'garden', '.'],\n",
       "  ['11:', 'John', 'moved', 'to', 'the', 'office', '.'],\n",
       "  ['12:', 'Daniel', 'went', 'back', 'to', 'the', 'kitchen', '.'],\n",
       "  ['13:', 'Mary', 'moved', 'to', 'the', 'kitchen', '.'],\n",
       "  ['14:', 'Mary', 'moved', 'to', 'the', 'hallway', '.'],\n",
       "  ['15:', 'Mary', 'went', 'to', 'the', 'kitchen', '.'],\n",
       "  ['16:', 'Sandra', 'went', 'back', 'to', 'the', 'bedroom', '.'],\n",
       "  ['17:', 'Sandra', 'travelled', 'to', 'the', 'hallway', '.'],\n",
       "  ['18:', 'Sandra', 'travelled', 'to', 'the', 'kitchen', '.'],\n",
       "  ['19:', 'Sandra', 'moved', 'to', 'the', 'garden', '.'],\n",
       "  ['20:', 'Daniel', 'went', 'to', 'the', 'garden', '.'],\n",
       "  ['21:', 'Sandra', 'went', 'back', 'to', 'the', 'bathroom', '.'],\n",
       "  ['22:', 'John', 'moved', 'to', 'the', 'garden', '.'],\n",
       "  ['23:', 'Mary', 'went', 'to', 'the', 'bathroom', '.'],\n",
       "  ['24:', 'Daniel', 'travelled', 'to', 'the', 'kitchen', '.'],\n",
       "  ['25:', 'John', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['26:', 'Sandra', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['27:', 'Mary', 'went', 'to', 'the', 'hallway', '.'],\n",
       "  ['28:', 'Daniel', 'went', 'back', 'to', 'the', 'garden', '.'],\n",
       "  ['29:', 'Sandra', 'went', 'back', 'to', 'the', 'office', '.'],\n",
       "  ['30:', 'Sandra', 'moved', 'to', 'the', 'kitchen', '.'],\n",
       "  ['31:', 'Mary', 'travelled', 'to', 'the', 'garden', '.'],\n",
       "  ['32:', 'Sandra', 'went', 'to', 'the', 'garden', '.'],\n",
       "  ['33:', 'Daniel', 'journeyed', 'to', 'the', 'hallway', '.'],\n",
       "  ['34:', 'Mary', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['35:', 'Daniel', 'travelled', 'to', 'the', 'garden', '.'],\n",
       "  ['36:', 'John', 'journeyed', 'to', 'the', 'bathroom', '.'],\n",
       "  ['37:', 'Daniel', 'travelled', 'to', 'the', 'hallway', '.'],\n",
       "  ['38:', 'Daniel', 'travelled', 'to', 'the', 'bedroom', '.'],\n",
       "  ['39:', 'Mary', 'went', 'back', 'to', 'the', 'kitchen', '.'],\n",
       "  ['40:', 'Daniel', 'went', 'to', 'the', 'office', '.'],\n",
       "  ['41:', 'John', 'journeyed', 'to', 'the', 'hallway', '.'],\n",
       "  ['42:', 'John', 'went', 'to', 'the', 'kitchen', '.'],\n",
       "  ['43:', 'Daniel', 'travelled', 'to', 'the', 'hallway', '.'],\n",
       "  ['44:', 'Sandra', 'went', 'back', 'to', 'the', 'kitchen', '.'],\n",
       "  ['45:', 'Mary', 'moved', 'to', 'the', 'office', '.'],\n",
       "  ['46:', 'Sandra', 'went', 'back', 'to', 'the', 'garden', '.'],\n",
       "  ['47:', 'Sandra', 'went', 'back', 'to', 'the', 'kitchen', '.'],\n",
       "  ['48:', 'Sandra', 'moved', 'to', 'the', 'garden', '.'],\n",
       "  ['49:', 'Sandra', 'moved', 'to', 'the', 'office', '.'],\n",
       "  ['50:', 'John', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['51:', 'Daniel', 'went', 'to', 'the', 'garden', '.'],\n",
       "  ['52:', 'Sandra', 'travelled', 'to', 'the', 'kitchen', '.'],\n",
       "  ['53:', 'Sandra', 'moved', 'to', 'the', 'bathroom', '.'],\n",
       "  ['54:', 'John', 'journeyed', 'to', 'the', 'garden', '.'],\n",
       "  ['55:', 'Mary', 'moved', 'to', 'the', 'hallway', '.'],\n",
       "  ['56:', 'John', 'went', 'back', 'to', 'the', 'office', '.'],\n",
       "  ['57:', 'Mary', 'went', 'back', 'to', 'the', 'office', '.'],\n",
       "  ['58:', 'Daniel', 'travelled', 'to', 'the', 'bathroom', '.'],\n",
       "  ['59:', 'Sandra', 'travelled', 'to', 'the', 'hallway', '.'],\n",
       "  ['60:', 'Sandra', 'journeyed', 'to', 'the', 'bathroom', '.'],\n",
       "  ['61:', 'Sandra', 'travelled', 'to', 'the', 'bedroom', '.'],\n",
       "  ['62:', 'Mary', 'went', 'back', 'to', 'the', 'hallway', '.'],\n",
       "  ['63:', 'Sandra', 'travelled', 'to', 'the', 'kitchen', '.'],\n",
       "  ['64:', 'Daniel', 'travelled', 'to', 'the', 'garden', '.'],\n",
       "  ['65:', 'Daniel', 'journeyed', 'to', 'the', 'bedroom', '.'],\n",
       "  ['66:', 'Mary', 'journeyed', 'to', 'the', 'office', '.'],\n",
       "  ['67:', 'Sandra', 'went', 'back', 'to', 'the', 'office', '.'],\n",
       "  ['68:', 'John', 'travelled', 'to', 'the', 'hallway', '.'],\n",
       "  ['69:', 'Daniel', 'picked', 'up', 'the', 'milk', 'there', '.'],\n",
       "  ['70:', 'Daniel', 'picked', 'up', 'the', 'apple', 'there', '.'],\n",
       "  ['71:', 'Sandra', 'moved', 'to', 'the', 'hallway', '.'],\n",
       "  ['72:', 'John', 'journeyed', 'to', 'the', 'bedroom', '.'],\n",
       "  ['73:', 'John', 'went', 'back', 'to', 'the', 'garden', '.'],\n",
       "  ['74:', 'Sandra', 'journeyed', 'to', 'the', 'office', '.'],\n",
       "  ['75:', 'Sandra', 'moved', 'to', 'the', 'bedroom', '.'],\n",
       "  ['76:', 'Mary', 'moved', 'to', 'the', 'kitchen', '.'],\n",
       "  ['77:', 'Mary', 'went', 'to', 'the', 'office', '.'],\n",
       "  ['78:', 'Sandra', 'grabbed', 'the', 'football', 'there', '.'],\n",
       "  ['79:', 'Sandra', 'discarded', 'the', 'football', '.'],\n",
       "  ['81:', 'Daniel', 'took', 'the', 'football', 'there', '.'],\n",
       "  ['82:', 'Daniel', 'put', 'down', 'the', 'apple', 'there', '.'],\n",
       "  ['84:', 'Daniel', 'took', 'the', 'apple', 'there', '.'],\n",
       "  ['85:', 'Daniel', 'travelled', 'to', 'the', 'hallway', '.'],\n",
       "  ['87:', 'Sandra', 'journeyed', 'to', 'the', 'bathroom', '.'],\n",
       "  ['88:', 'Daniel', 'left', 'the', 'milk', 'there', '.'],\n",
       "  ['90:', 'Daniel', 'went', 'to', 'the', 'kitchen', '.'],\n",
       "  ['91:', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.']],\n",
       " ['Where', 'is', 'the', 'milk', '?'],\n",
       " 'hallway')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stories[534]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the question \"Where is the milk?\" requires to supporting facts to answer, \"Daniel traveled to the hallway\" and \"Daniel left the milk there\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again) Here we calculate upper bounds for things like words in sentence, sentences in a story, etc. for the corpus, which will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories = train_stories + test_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['0:', 'Sandra', 'journeyed', 'to', 'the', 'kitchen', '.'],\n",
       "  ['1:', 'Daniel', 'travelled', 'to', 'the', 'office', '.'],\n",
       "  ['2:', 'John', 'travelled', 'to', 'the', 'bathroom', '.'],\n",
       "  ['3:', 'Sandra', 'moved', 'to', 'the', 'bathroom', '.'],\n",
       "  ['4:', 'Mary', 'went', 'back', 'to', 'the', 'garden', '.'],\n",
       "  ['5:', 'Mary', 'grabbed', 'the', 'milk', 'there', '.'],\n",
       "  ['6:', 'Mary', 'left', 'the', 'milk', '.'],\n",
       "  ['7:', 'Sandra', 'journeyed', 'to', 'the', 'garden', '.'],\n",
       "  ['9:', 'Mary', 'journeyed', 'to', 'the', 'bathroom', '.'],\n",
       "  ['10:', 'Sandra', 'took', 'the', 'milk', 'there', '.'],\n",
       "  ['11:', 'Daniel', 'moved', 'to', 'the', 'bedroom', '.'],\n",
       "  ['12:', 'Daniel', 'journeyed', 'to', 'the', 'kitchen', '.'],\n",
       "  ['13:', 'Sandra', 'discarded', 'the', 'milk', '.'],\n",
       "  ['14:', 'Daniel', 'went', 'back', 'to', 'the', 'bathroom', '.'],\n",
       "  ['16:', 'John', 'travelled', 'to', 'the', 'kitchen', '.'],\n",
       "  ['17:', 'Daniel', 'went', 'to', 'the', 'kitchen', '.'],\n",
       "  ['19:', 'Mary', 'went', 'back', 'to', 'the', 'garden', '.'],\n",
       "  ['20:', 'Daniel', 'grabbed', 'the', 'football', 'there', '.'],\n",
       "  ['21:', 'Daniel', 'moved', 'to', 'the', 'bedroom', '.'],\n",
       "  ['22:', 'Sandra', 'grabbed', 'the', 'milk', 'there', '.'],\n",
       "  ['24:', 'Sandra', 'put', 'down', 'the', 'milk', '.'],\n",
       "  ['25:', 'Sandra', 'got', 'the', 'milk', 'there', '.']],\n",
       " ['Where', 'is', 'the', 'football', '?'],\n",
       " 'bedroom')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[534]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "story_maxlen = max(len(s) for x, _, _ in stories for s in x)\n",
    "story_maxsents = max(len(x) for x, _, _ in stories)\n",
    "query_maxlen = max(len(x) for _, x, _ in stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_flatten(el):\n",
    "    return isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes))\n",
    "\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if do_flatten(el): yield from flatten(el)\n",
    "        else: yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again) Create vocabulary of corpus and find size, including a padding element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(flatten(stories)))\n",
    "vocab.insert(0, '<PAD>')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 124, 8, 5, 10000, 1000)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_maxsents, vocab_size, story_maxlen, query_maxlen, len(train_stories), len(test_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again) Create an index mapping for the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx = dict((c,i) for i,c in enumerate(vocab))\n",
    "#word_idx = {c:i for i,c in enumerate(vocab)}　#same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again) Next we vectorize our dataset by mapping words to their indices. We enforce consistent dimension by padding vectors up to the upper bounds we calculated earlier with our pad element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories, \n",
    "     word_idx, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, (16, 8), (18, 8))"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs_train), inputs_train[2].shape, inputs_train[-2].shape  # inputs_train shape: 10000 * [sent, 8] arrays  \n",
    "                                                                  # m = len(train_stories): 10000\n",
    "                                                                  # story_maxlen (maximu words per sentence): 8\n",
    "                                                                  # sent (per story): not fixed number < story_maxsents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   2,  91, 123, 119, 117, 110,   1],\n",
       "       [  0,  13,  91, 109, 119, 117, 107,   1],\n",
       "       [ 24,  93, 123,  97, 119, 117, 104,   1],\n",
       "       [ 35,  91, 115, 122, 117,  96, 118,   1],\n",
       "       [  0,  46,  94, 123, 119, 117, 114,   1],\n",
       "       [  0,  57,  94, 121, 119, 117,  99,   1],\n",
       "       [  0,  68,  93, 105, 117, 103, 118,   1],\n",
       "       [  0,  79,  94, 106, 117, 112, 118,   1],\n",
       "       [  0,   0,  86,  93, 111, 117, 103,   1],\n",
       "       [  0,   0,  89,  91, 111, 117,  96,   1]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stack_inputs(inputs):\n",
    "    for i, it in enumerate(inputs):\n",
    "        inputs[i] = np.concatenate([it, \n",
    "                           np.zeros((story_maxsents-it.shape[0],story_maxlen), 'int')])\n",
    "    return np.stack(inputs)\n",
    "\n",
    "inputs_train = stack_inputs(inputs_train)\n",
    "inputs_test = stack_inputs(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   2,  91, 123, 119, 117, 110,   1],\n",
       "       [  0,  13,  91, 109, 119, 117, 107,   1],\n",
       "       [ 24,  93, 123,  97, 119, 117, 104,   1],\n",
       "       [ 35,  91, 115, 122, 117,  96, 118,   1],\n",
       "       [  0,  46,  94, 123, 119, 117, 114,   1],\n",
       "       [  0,  57,  94, 121, 119, 117,  99,   1],\n",
       "       [  0,  68,  93, 105, 117, 103, 118,   1],\n",
       "       [  0,  79,  94, 106, 117, 112, 118,   1],\n",
       "       [  0,   0,  86,  93, 111, 117, 103,   1],\n",
       "       [  0,   0,  89,  91, 111, 117,  96,   1],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 88, 8)\n",
      "(1000, 88, 8)\n"
     ]
    }
   ],
   "source": [
    "print(inputs_train.shape) # inputs_train shape: [m = len(train_stories), story_maxsents, story_maxlen] arrays       \n",
    "print(inputs_test.shape)  # inputs_test shape: [m = len(test_stories), story_maxsents, story_maxlen] arrays                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n",
      "(1000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(queries_train.shape) # queries_train shape: [m = len(train_stories), query_maxlen] arrays       \n",
    "print(queries_test.shape) # queries_test shape: [m = len(train_stories), query_maxlen] arrays    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again) Our inputs for keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inps = [inputs_train, queries_train]  # train dataset = [inputs_train, queries_train]\n",
    "val_inps = [inputs_test, queries_test] # val dataset = [inputs_test, queries_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again) Our dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(answers_train.shape)\n",
    "print(answers_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi hop Model\n",
    "The approach is basically the same; we add more embedding dimensions to account for the increased task complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 30\n",
    "params = {'verbose': 2, 'callbacks': [TQDMNotebookCallback(leave_inner=False)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emb_sent_bow(inp):\n",
    "    emb_op = TimeDistributed(Embedding(vocab_size, emb_dim)) \n",
    "    emb = emb_op(inp)                                       # emb.shape: [m, story_maxsents=10, story_maxlen=88, emb_dim=30]\n",
    "                                                            # TimeDistributed() seems to be for this first inp dim: 'story_maxsents=88'\n",
    "    #print('emb.shape:', repr(emb.shape))\n",
    "    emb = Lambda(lambda x: K.sum(x, axis=2))(emb)    # the Lambda layer adds up all embedding (in shape [m, story_maxsents=88, emb_dim=30])\n",
    "    return emb, emb_op  # Difference on Multi Hop: this time we output also emb_op to use for inp_q outputing emb_q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_story.shape: TensorShape([Dimension(None), Dimension(88), Dimension(8)])\n",
      "emb_story.shape: TensorShape([Dimension(None), Dimension(88), Dimension(30)])\n"
     ]
    }
   ],
   "source": [
    "inp_story = Input((story_maxsents, story_maxlen))  # TimeDistributed() is for this first inp_story dim: 'story_maxsents=88'\n",
    "emb_story, emb_story_op = emb_sent_bow(inp_story)\n",
    "print('inp_story.shape:', repr(inp_story.shape))  # inp_story.shape: [m, story_maxsents=88, story_maxlen=8]\n",
    "print('emb_story.shape:', repr(emb_story.shape))  # emb_story.shape: [m, story_maxsents=88, emb_dim=30    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_q.shape TensorShape([Dimension(None), Dimension(5), Dimension(30)])\n",
      "inp_q.shape: TensorShape([Dimension(None), Dimension(5)])\n",
      "emb_q.shape TensorShape([Dimension(None), Dimension(30)])\n"
     ]
    }
   ],
   "source": [
    "inp_q = Input((query_maxlen,))\n",
    "emb_q = emb_story_op.layer(inp_q) # emb_q.shape: [m, query_maxlen=5, emb_dim=30]\n",
    "print('emb_q.shape', repr(emb_q.shape))\n",
    "emb_q = Lambda(lambda x: K.sum(x, axis=1))(emb_q) # # the Lambda layer adds up all embedding (in shape [m, emb_dim=30])\n",
    "print('inp_q.shape:', repr(inp_q.shape)) \n",
    "print('emb_q.shape', repr(emb_q.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is that we are going to do the same process twice. Here we've defined a \"hop\" as the operation that returns the weighted average of the input sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hop = Dense(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hop(u, A):                \n",
    "    '''\n",
    "       u: query question\n",
    "       A: emb_story\n",
    "    '''\n",
    "    x = Reshape((1, emb_dim))(u)   # Difference\n",
    "    x = Dot(axes=2)([A, x])        # Difference\n",
    "    #print(repr(x.shape))              # x.shape: [m, story_maxsents=88, 1]  \n",
    "    x = Reshape((story_maxsents,))(x)  # Softmax works on the last dimension, so I have to Reshape to get rid of the unit axis \n",
    "                                       # and then I Reshape again to put the unit axis back on again.\n",
    "    x = Activation('softmax')(x)\n",
    "    #print(repr(x.shape))              # x.shape: [m, story_maxsents=88]  \n",
    "    match = Reshape((story_maxsents, 1))(x)  # match: how similar the query is to each sentence.\n",
    "    #print(repr(match.shape))     # match.shape: [m, story_maxsents=88, 1]\n",
    "    \n",
    "    emb_c, _ = emb_sent_bow(inp_story)  # emb_c.shape: [m, story_maxsents=88, emb_dim=30]\n",
    "    x = Dot(axes=1)([match, emb_c])\n",
    "    #print(repr(x.shape))             # x.shape: [m, 1, emb_dim=30]\n",
    "    x = Reshape((emb_dim,))(x)      \n",
    "    x = hop(x)                      # Main Difference; # x.shape: [m, emb_dim=30]\n",
    "    #print(repr(x.shape))\n",
    "    x = Add()([x, emb_q])           # Difference\n",
    "    return x, emb_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ref] Here is above one-hop model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = Dot(axes=2)([emb_story, emb_q]) \n",
    "#print(repr(x.shape))              # x.shape: [m, story_maxsents=10, 1]  \n",
    "x = Reshape((story_maxsents,))(x)\n",
    "x = Activation('softmax')(x)\n",
    "#print(repr(x.shape))              # x.shape: [m, story_maxsents=10]  \n",
    "match = Reshape((story_maxsents, 1))(x)  # match: how similar the query is to each sentence.\n",
    "print(repr(match.shape))     # match.shape: [m, story_maxsents=10, 1]\n",
    "\n",
    "emb_c = emb_sent_bow(inp_story)  # emb_c.shape: [m, story_maxsents=10, emb_dim=20]\n",
    "x = Dot(axes=1)([match, emb_c])\n",
    "print(repr(x.shape))             # x.shape: [m, 1, emb_dim=20]\n",
    "response = Reshape((emb_dim,))(x)\n",
    "res = Dense(vocab_size, activation='softmax')(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do one hop, and repeat the process using the resulting weighted sentence average as the new weights.\n",
    "\n",
    "This works because the first hop allows us to find the first fact relevant to the query, and then we can use that fact to find the next fact that answers the question. In our example, our model would first find the last sentence to mention \"milk\", and then use the information in that fact to know that it next has to find the last occurrence of \"Daniel\".\n",
    "\n",
    "This is facilitated by generating a new embedding function for the input story each time we hop. This means that the first embedding is learning things that help us find the first fact from the query, and the second is helping us find the second fact from the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be extended to n-supporting factor problems by doing n hops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([Dimension(None), Dimension(88), Dimension(1)])\n",
      "TensorShape([Dimension(None), Dimension(88)])\n",
      "TensorShape([Dimension(None), Dimension(88), Dimension(1)])\n",
      "TensorShape([Dimension(None), Dimension(1), Dimension(30)])\n",
      "TensorShape([Dimension(None), Dimension(30)])\n",
      "TensorShape([Dimension(None), Dimension(88), Dimension(1)])\n",
      "TensorShape([Dimension(None), Dimension(88)])\n",
      "TensorShape([Dimension(None), Dimension(88), Dimension(1)])\n",
      "TensorShape([Dimension(None), Dimension(1), Dimension(30)])\n",
      "TensorShape([Dimension(None), Dimension(30)])\n"
     ]
    }
   ],
   "source": [
    "response, emb_story = one_hop(emb_q, emb_story)     # use emb_q as question to find 1st response\n",
    "response, emb_story = one_hop(response, emb_story)  # use 1st response as question to find 2nd response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = Dense(vocab_size, activation='softmax')(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer = Model([inp_story, inp_q], res)\n",
    "answer.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting this model can be tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12666dc13cd84891bbd8da97cbbfb8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1064d782efe54007a554077e2795b55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "11s - loss: 1.7038 - acc: 0.2950 - val_loss: 1.4643 - val_acc: 0.4250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426aa6c8a5a64502862b3c241ca17ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "10s - loss: 0.9887 - acc: 0.6355 - val_loss: 0.9092 - val_acc: 0.6820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea32321e77f5447ba871cc0ed290f200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "9s - loss: 0.7793 - acc: 0.7242 - val_loss: 0.7466 - val_acc: 0.7180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94aaddcf63c45bc8d2f738947d9c535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "11s - loss: 0.6805 - acc: 0.7673 - val_loss: 0.7754 - val_acc: 0.7590\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157d7ed036474cc4b7f286828d9c80c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "10s - loss: 0.6356 - acc: 0.7842 - val_loss: 0.6768 - val_acc: 0.7600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbb979ca58743baa2b5a0ad6136ab9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n",
      "9s - loss: 0.6000 - acc: 0.8065 - val_loss: 0.7982 - val_acc: 0.7380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0431eb7f14f34dca8e265dfeb665a3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n",
      "9s - loss: 0.5961 - acc: 0.8137 - val_loss: 0.7440 - val_acc: 0.7950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66d0bbe45564bc5ab01f1bb177b28c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "9s - loss: 0.5840 - acc: 0.8246 - val_loss: 0.7258 - val_acc: 0.7920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.set_value(answer.optimizer.lr, 5e-3)\n",
    "hist = answer.fit(inps, answers_train, **params, epochs=8, batch_size=batch_size,\n",
    "                 validation_data=(val_inps, answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.425, 0.682, 0.718, 0.759, 0.76 , 0.738, 0.795, 0.792])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
