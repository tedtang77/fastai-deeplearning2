{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "import utils2; reload(utils2)\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch_utils; reload(torch_utils)\n",
    "from torch_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that in the last month the GAN training problem has been solved! [This paper](https://arxiv.org/abs/1701.07875) shows a minor change to the loss function and constraining the weights allows a GAN to reliably learn following a consistent loss schedule.\n",
    "\n",
    "First, we, set up batch size, image size, and size of noise vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs, sz, nz = 64, 64, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has the handy [torch-vision](https://github.com/pytorch/vision) library which makes handling images fast and easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "PATH = '../data2/cifar10/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Scale(sz),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data = datasets.CIFAR10(root=PATH, download=True,\n",
    "                       transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "../data2/lsun/bedroom_train_lmdb: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-fea90c030e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSUN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bedroom_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/lsun.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, db_path, classes, transform, target_transform)\u001b[0m\n\u001b[1;32m    104\u001b[0m             self.dbs.append(LSUNClass(\n\u001b[1;32m    105\u001b[0m                 \u001b[0mdb_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_lmdb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 transform=transform))\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/lsun.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, db_path, transform, target_transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         self.env = lmdb.open(db_path, max_readers=1, readonly=True, lock=False,\n\u001b[0;32m---> 19\u001b[0;31m                              readahead=False, meminit=False)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entries'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: ../data2/lsun/bedroom_train_lmdb: No such file or directory"
     ]
    }
   ],
   "source": [
    "PATH = '../data2/lsun'\n",
    "#os.makedirs(PATH+'bedroom_train_lmdb', exist_ok=True)\n",
    "transform=transforms.Compose([\n",
    "        transforms.Scale(sz),\n",
    "        transforms.CenterCrop(sz),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data = datasets.LSUN(db_path=PATH, classes=['bedroom_train'], transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even parallel processing is handling automatically by torch-vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(data, batch_size=bs,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "n = len(dataloader); n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our activation function will be `tanh`, so we need to do some processing to view the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(img, fs=(6,6)):\n",
    "    plt.figure(figsize = fs)\n",
    "    plt.imshow(np.transpose((img/2+0.5).clamp(0,1).numpy(), (1, 2, 0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN definitions are a little big for a notebook, so we import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dcgan_prac1; reload(dcgan_prac1)\n",
    "from dcgan_prac1 import DCGAN_D, DCGAN_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch uses `module.apply()` for picking an initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        m.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.normal_(mean=1.0, std=0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCGAN_G(\n",
       "  (main): Sequential(\n",
       "    (initial-100.512.convt): ConvTranspose2d (100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (initial-512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (initial-512.relu): ReLU(inplace)\n",
       "    (pyramid-512.256.convt): ConvTranspose2d (512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid-256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid-256.relu): ReLU(inplace)\n",
       "    (pyramid-256.128.convt): ConvTranspose2d (256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid-128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid-128.relu): ReLU(inplace)\n",
       "    (pyramid-128.64.convt): ConvTranspose2d (128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid-64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid-64.relu): ReLU(inplace)\n",
       "    (extra-0-64.64.convt): ConvTranspose2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (extra-0-64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (extra-0-64.relu): ReLU(inplace)\n",
       "    (final.64-3.convt): ConvTranspose2d (64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (final.3.tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DCGAN_G(isize, nz, nc, ngf, ngpu, n_extra_layers)\n",
    "netG = DCGAN_G(sz, nz, 3, 64, 1, 1).cuda()\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCGAN_D(\n",
       "  (main): Sequential(\n",
       "    (initial-3.64.conv): Conv2d (3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (initial-64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (initial-64.relu): LeakyReLU(0.2, inplace)\n",
       "    (extra-0-64.64.conv): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (extra-0-64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (extra-0-64.relu): LeakyReLU(0.2, inplace)\n",
       "    (pyramid-64.128.conv): Conv2d (64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid-128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid-128.relu): LeakyReLU(0.2, inplace)\n",
       "    (pyramid-128.256.conv): Conv2d (128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid-256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid-256.relu): LeakyReLU(0.2, inplace)\n",
       "    (pyramid-256.512.conv): Conv2d (256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid-512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid-512.relu): LeakyReLU(0.2, inplace)\n",
       "    (final.512-1.conv): Conv2d (512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DCGAN_D(isize, nc, ndf, ngpu, n_extra_layers)\n",
    "netD = DCGAN_D(sz, 3, 64, 1, 1).cuda()\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some shortcuts to create tensors and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import FloatTensor as FT\n",
    "def Var(*params): return Variable(FT(*params).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_noise(b):\n",
    "    return Variable(FT(b, nz, 1, 1).cuda().normal_(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input placeholder\n",
    "input = Var(bs, 3, sz, nz)\n",
    "# Fixed noise used just for visualizing images when done\n",
    "fixed_noise = create_noise(bs)\n",
    "# The numbers 0 and -1\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer needs to be told what variables to optimize. A module automatically keeps track of its variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.RMSprop(netD.parameters(), lr = 1e-4)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One forward step and one backward step for D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_D(v, init_grad):\n",
    "    err = netD(v)\n",
    "    err.backward(init_grad)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trainable(net, val):\n",
    "    for p in net.parameters(): p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(niter, first=True):\n",
    "    gen_iterations = 0\n",
    "    for epoch in range(niter):\n",
    "        data_iter = iter(dataloader)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            make_trainable(netD, True)\n",
    "            d_iters = (100 if first and (gen_iterations<25) or (gen_iterations % 500 == 0) \n",
    "                      else 5) \n",
    "            \n",
    "            j = 0\n",
    "            while j < d_iters and i < n:\n",
    "                j += 1; i += 1\n",
    "                for p in netD.parameters(): p.data.clamp_(-0.01, 0.01)\n",
    "                real = Variable(next(data_iter)[0].cuda())\n",
    "                netD.zero_grad()\n",
    "                errD_real = step_D(real, one)\n",
    "                \n",
    "                fake = netG(create_noise(real.size()[0]))\n",
    "                input.data.resize_(real.size()).copy_(fake.data)\n",
    "                errD_fake = step_D(input, mone)\n",
    "                errD = errD_real - errD_fake\n",
    "                optimizerD.step()\n",
    "            \n",
    "            make_trainable(netD, False)\n",
    "            netG.zero_grad()\n",
    "            errG = step_D(netG(create_noise(bs)), one)\n",
    "            optimizerG.step()\n",
    "            gen_iterations += 1\n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f' % (\n",
    "            epoch, niter, gen_iterations, n,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][8/782] Loss_D: -1.556155 Loss_G: 0.755735 Loss_D_real: -0.814974 Loss_D_fake 0.741182\n",
      "[1/200][16/782] Loss_D: -1.563625 Loss_G: 0.758643 Loss_D_real: -0.819034 Loss_D_fake 0.744591\n",
      "[2/200][24/782] Loss_D: -1.567824 Loss_G: 0.758920 Loss_D_real: -0.823326 Loss_D_fake 0.744498\n",
      "[3/200][162/782] Loss_D: -1.441777 Loss_G: 0.676901 Loss_D_real: -0.780393 Loss_D_fake 0.661383\n",
      "[4/200][319/782] Loss_D: -1.505537 Loss_G: 0.700469 Loss_D_real: -0.819340 Loss_D_fake 0.686197\n",
      "[5/200][476/782] Loss_D: -1.484864 Loss_G: 0.707040 Loss_D_real: -0.796190 Loss_D_fake 0.688674\n",
      "[6/200][614/782] Loss_D: -1.455112 Loss_G: 0.716488 Loss_D_real: -0.795500 Loss_D_fake 0.659612\n",
      "[7/200][771/782] Loss_D: -1.510170 Loss_G: 0.724779 Loss_D_real: -0.793192 Loss_D_fake 0.716978\n",
      "[8/200][928/782] Loss_D: -0.027120 Loss_G: -0.540400 Loss_D_real: -0.668299 Loss_D_fake -0.641179\n",
      "[9/200][1066/782] Loss_D: -0.466443 Loss_G: -0.284900 Loss_D_real: 0.172127 Loss_D_fake 0.638570\n",
      "[10/200][1223/782] Loss_D: -1.166974 Loss_G: 0.069210 Loss_D_real: -0.492047 Loss_D_fake 0.674927\n",
      "[11/200][1380/782] Loss_D: -0.717545 Loss_G: 0.613012 Loss_D_real: -0.617093 Loss_D_fake 0.100451\n",
      "[12/200][1518/782] Loss_D: -0.214414 Loss_G: 0.399444 Loss_D_real: -0.738831 Loss_D_fake -0.524418\n",
      "[13/200][1675/782] Loss_D: -1.334031 Loss_G: 0.594886 Loss_D_real: -0.693313 Loss_D_fake 0.640718\n",
      "[14/200][1832/782] Loss_D: -1.043735 Loss_G: 0.233552 Loss_D_real: -0.423837 Loss_D_fake 0.619898\n",
      "[15/200][1989/782] Loss_D: -1.274465 Loss_G: 0.607363 Loss_D_real: -0.673675 Loss_D_fake 0.600790\n",
      "[16/200][2127/782] Loss_D: -0.518916 Loss_G: 0.295623 Loss_D_real: 0.129480 Loss_D_fake 0.648396\n",
      "[17/200][2284/782] Loss_D: -1.353428 Loss_G: 0.653293 Loss_D_real: -0.721582 Loss_D_fake 0.631845\n",
      "[18/200][2441/782] Loss_D: -1.092167 Loss_G: 0.592387 Loss_D_real: -0.609690 Loss_D_fake 0.482477\n",
      "[19/200][2579/782] Loss_D: -0.769080 Loss_G: 0.639088 Loss_D_real: -0.700221 Loss_D_fake 0.068859\n",
      "[20/200][2736/782] Loss_D: -1.075558 Loss_G: 0.226408 Loss_D_real: -0.466314 Loss_D_fake 0.609244\n",
      "[21/200][2893/782] Loss_D: -1.349022 Loss_G: 0.655794 Loss_D_real: -0.706374 Loss_D_fake 0.642647\n"
     ]
    }
   ],
   "source": [
    "%time train(200, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake = netG(fixed_noise).data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show(vutils.make_grid(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show(vutils.make_grid(iter(dataloader).next()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show(vutils.make_grid(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show(vutils.make_grid(iter(dataloader).next()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
